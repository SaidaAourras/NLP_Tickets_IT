# ğŸ« NLP Pipeline for IT Support Ticket Classification

[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](https://www.docker.com/)
[![Kubernetes](https://img.shields.io/badge/kubernetes-compatible-326CE5.svg)](https://kubernetes.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> An end-to-end MLOps pipeline for automated classification of IT support tickets using Natural Language Processing, featuring real-time monitoring and drift detection.

![Pipeline Architecture](docs/images/architecture.png)

---

## ğŸ“– Overview

This project implements a production-ready, batch-processing NLP pipeline that automates the classification of IT support tickets. Built with industry best practices, it combines state-of-the-art machine learning with robust monitoring and containerized deployment.

### ğŸ¯ Key Features

- âœ¨ **Semantic Understanding**: Leverages Hugging Face transformers for deep text comprehension
- ğŸ”„ **Vector Storage**: ChromaDB integration for efficient embedding management
- ğŸ“Š **Model Monitoring**: Evidently AI for data drift and prediction drift detection
- ğŸ³ **Containerized**: Fully Dockerized for consistent deployment across environments
- â˜¸ï¸ **Cloud-Ready**: Kubernetes-orchestrated with support for batch jobs and cron schedules
- ğŸ“ˆ **Infrastructure Monitoring**: Prometheus + Grafana stack for real-time metrics
- ğŸ” **CI/CD**: Automated testing and deployment with GitHub Actions

### ğŸ’¼ Business Impact

- **80%+ automation** of ticket classification
- **Reduced response time** through intelligent routing
- **Early detection** of model degradation
- **Scalable architecture** for growing support volumes

---

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA INGESTION                            â”‚
â”‚                    (IT Support Emails)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NLP PREPROCESSING                            â”‚
â”‚  â€¢ Text normalization  â€¢ Tokenization  â€¢ Stopword removal       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBEDDING GENERATION                          â”‚
â”‚              (Hugging Face Transformers)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â–¼                  â–¼                             â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ChromaDB   â”‚   â”‚ Classificationâ”‚          â”‚  Evidently   â”‚
         â”‚   (Vector    â”‚   â”‚    Model      â”‚          â”‚      AI      â”‚
         â”‚   Storage)   â”‚   â”‚  (Scikit-learn)â”‚         â”‚  (Monitoring)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Predictions â”‚
                            â”‚   & Reports  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ Technology Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **ML Framework** | Hugging Face Transformers | Pre-trained NLP models for embeddings |
| **Classification** | Scikit-learn | Supervised learning algorithms |
| **Vector Database** | ChromaDB | Semantic search and embedding storage |
| **ML Monitoring** | Evidently AI | Drift detection and model quality tracking |
| **Containerization** | Docker | Application packaging and isolation |
| **Orchestration** | Kubernetes (Minikube) | Container orchestration and scheduling |
| **Metrics** | Prometheus | Time-series metrics collection |
| **Visualization** | Grafana | Real-time dashboards and alerting |
| **Container Monitoring** | cAdvisor | Docker container resource usage |
| **Node Monitoring** | Node Exporter | Host machine metrics |
| **CI/CD** | GitHub Actions | Automated testing and deployment |
| **Language** | Python 3.9+ | Core development language |

---

## ğŸš€ Quick Start

### Prerequisites

Ensure you have the following installed:
- Python 3.9 or higher
- Docker & Docker Compose
- Kubernetes (Minikube for local development)
- Git

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/your-username/nlp-ticket-classification.git
cd nlp-ticket-classification
```

2. **Set up Python environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. **Download sample data** (if applicable)
```bash
python scripts/download_data.py
```

4. **Run the pipeline**
```bash
python main.py --config config/default.yaml
```

### Docker Deployment
```bash
# Build the Docker image
docker build -t nlp-pipeline:latest .

# Run the container
docker run --rm \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/models:/app/models \
  nlp-pipeline:latest
```

### Kubernetes Deployment
```bash
# Start Minikube
minikube start --memory=4096 --cpus=2

# Deploy the pipeline
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/job.yaml

# Monitor execution
kubectl get pods -n ml-pipeline
kubectl logs -f <pod-name> -n ml-pipeline

# Schedule recurring runs
kubectl apply -f k8s/cronjob.yaml
```

### Monitoring Stack
```bash
# Launch Prometheus, Grafana, and exporters
docker-compose up -d

# Access dashboards
# Prometheus: http://localhost:9090
# Grafana:    http://localhost:3000 (admin/admin)
# cAdvisor:   http://localhost:8080

# Import Grafana dashboards
# Navigate to Grafana > Dashboards > Import
# Use dashboard IDs: 893 (Main), 179 (Docker)
```

---

## ğŸ“Š Pipeline Stages

### 1ï¸âƒ£ Data Preprocessing

**Objective:** Transform raw email data into clean, structured text

**Process:**
- Merge `subject` and `body` fields
- Convert to lowercase
- Remove punctuation and special characters
- Tokenize text
- Filter stopwords

**Output:** Cleaned text corpus ready for embedding generation

### 2ï¸âƒ£ Embedding Generation

**Objective:** Convert text into dense vector representations

**Implementation:**
```python
from transformers import AutoTokenizer, AutoModel
import torch

# Load pre-trained model
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Generate embeddings
def get_embeddings(texts):
    encoded = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**encoded)
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.numpy()
```

**Output:** Normalized vector embeddings stored in ChromaDB

### 3ï¸âƒ£ Model Training

**Objective:** Train classification model to predict ticket categories

**Approach:**
- Train/test split (80/20)
- Multiple classifier evaluation (Logistic Regression, Random Forest, SVM)
- Hyperparameter tuning with GridSearchCV
- Cross-validation for robust performance estimation

**Metrics Tracked:**
- Accuracy
- Precision, Recall, F1-Score (per class)
- Confusion Matrix
- ROC-AUC

### 4ï¸âƒ£ Model Monitoring (Evidently AI)

**Objective:** Detect model degradation and data drift

**Implementation:**
```python
from evidently import Report
from evidently.metrics import DataDriftPreset, ClassificationPreset

# Generate comprehensive drift report
report = Report(metrics=[
    DataDriftPreset(),
    ClassificationPreset()
])

report.run(reference_data=baseline_df, current_data=production_df)
report.save_html('reports/drift_report.html')
```

**Monitored Aspects:**
- **Data Drift**: Statistical changes in feature distributions
- **Prediction Drift**: Shift in model output patterns
- **Target Drift**: Changes in label distribution
- **Model Quality**: Performance degradation over time

### 5ï¸âƒ£ Infrastructure Monitoring

**Objective:** Ensure system health and resource optimization

**Metrics Collected:**
- **Node Exporter**: CPU, memory, disk I/O, network
- **cAdvisor**: Per-container resource utilization
- **Custom Metrics**: Job duration, success/failure rates

**Sample Prometheus Query:**
```promql
# Container CPU usage
rate(container_cpu_usage_seconds_total[5m])

# Memory usage percentage
(container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100
```

---

## ğŸ“ Project Structure
```
nlp-ticket-classification/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                    # Continuous Integration
â”‚       â””â”€â”€ cd.yml                    # Continuous Deployment
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Original email datasets
â”‚   â”œâ”€â”€ processed/                    # Cleaned and tokenized data
â”‚   â””â”€â”€ embeddings/                   # Generated vector representations
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ classifier/
â”‚   â”‚   â”œâ”€â”€ model.pkl                 # Trained classification model
â”‚   â”‚   â””â”€â”€ metrics.json              # Model performance metrics
â”‚   â””â”€â”€ embeddings/
â”‚       â””â”€â”€ encoder_config.json       # Hugging Face model configuration
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_preprocessing.py           # Data cleaning and preparation
â”‚   â”œâ”€â”€ 02_embeddings.py              # Vector generation with Hugging Face
â”‚   â”œâ”€â”€ 03_training.py                # Model training and evaluation
â”‚   â”œâ”€â”€ 04_monitoring.py              # Evidently AI drift detection
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ text_processing.py        # NLP utilities
â”‚       â””â”€â”€ metrics.py                # Custom evaluation metrics
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb                  # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_model_selection.ipynb      # Algorithm comparison
â”‚   â””â”€â”€ 03_drift_analysis.ipynb       # Monitoring insights
â”‚
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ namespace.yaml                # Kubernetes namespace
â”‚   â”œâ”€â”€ configmap.yaml                # Configuration management
â”‚   â”œâ”€â”€ secret.yaml                   # Sensitive data (gitignored)
â”‚   â”œâ”€â”€ job.yaml                      # One-time pipeline execution
â”‚   â”œâ”€â”€ cronjob.yaml                  # Scheduled batch processing
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ prometheus-config.yaml
â”‚       â””â”€â”€ grafana-dashboards.json
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ drift_reports/                # Evidently AI HTML reports
â”‚   â”œâ”€â”€ performance_logs/             # Model evaluation history
â”‚   â””â”€â”€ monitoring_snapshots/         # Grafana exports
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                         # Unit tests
â”‚   â”œâ”€â”€ integration/                  # Integration tests
â”‚   â””â”€â”€ test_pipeline.py              # End-to-end pipeline test
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default.yaml                  # Default configuration
â”‚   â”œâ”€â”€ production.yaml               # Production settings
â”‚   â””â”€â”€ monitoring.yaml               # Prometheus/Grafana config
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ images/                       # Architecture diagrams
â”‚   â”œâ”€â”€ api.md                        # API documentation
â”‚   â””â”€â”€ deployment.md                 # Deployment guide
â”‚
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile                        # Multi-stage Docker build
â”œâ”€â”€ docker-compose.yml                # Monitoring stack orchestration
â”œâ”€â”€ prometheus.yml                    # Prometheus configuration
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ requirements-dev.txt              # Development dependencies
â”œâ”€â”€ setup.py                          # Package installation
â”œâ”€â”€ main.py                           # Pipeline entry point
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ”¬ Model Performance

### Classification Results

| Metric | Score |
|--------|-------|
| **Accuracy** | 87.3% |
| **Macro F1-Score** | 85.1% |
| **Weighted F1-Score** | 87.0% |

### Per-Class Performance

| Ticket Type | Precision | Recall | F1-Score | Support |
|-------------|-----------|--------|----------|---------|
| Hardware | 0.89 | 0.85 | 0.87 | 234 |
| Software | 0.88 | 0.91 | 0.89 | 312 |
| Network | 0.84 | 0.82 | 0.83 | 189 |
| Access | 0.86 | 0.89 | 0.87 | 265 |

### Embedding Model

- **Model**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimension**: 384
- **Speed**: ~500 sentences/sec (CPU)

---

## ğŸ“Š Monitoring & Observability

### ML Monitoring Dashboard

Evidently AI provides automated drift detection with:

- **Data Drift Score**: Kolmogorov-Smirnov test for distribution changes
- **Prediction Drift**: Jensen-Shannon divergence of output distributions
- **Feature Importance**: Identification of drifting features
- **Interactive Visualizations**: HTML reports with drill-down capabilities

**Access Reports:**
```bash
open reports/drift_reports/latest.html
```

### Infrastructure Monitoring

**Prometheus Queries:**
```promql
# Pipeline job success rate
sum(rate(job_success_total[1h])) / sum(rate(job_runs_total[1h]))

# Average job duration
histogram_quantile(0.95, rate(job_duration_seconds_bucket[5m]))

# Memory pressure
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes
```

**Pre-configured Grafana Dashboards:**
- **ML Pipeline Overview**: Job metrics, success rates, duration trends
- **Container Health**: CPU, memory, network per container
- **Node Resources**: System-level metrics

---

## ğŸ”„ CI/CD Pipeline

### GitHub Actions Workflow

**Triggered on:**
- Push to `main` or `develop` branches
- Pull requests
- Manual workflow dispatch

**Pipeline Stages:**
```yaml
name: ML Pipeline CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Lint with flake8
        run: |
          pip install flake8
          flake8 . --count --max-line-length=100 --statistics
      - name: Check with black
        run: |
          pip install black
          black --check .

  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install -r requirements.txt -r requirements-dev.txt
      - name: Run unit tests
        run: pytest tests/unit --cov=scripts --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v3

  build:
    needs: [lint, test]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Build Docker image
        run: docker build -t nlp-pipeline:${{ github.sha }} .
      - name: Run integration tests
        run: |
          docker run --rm nlp-pipeline:${{ github.sha }} python -m pytest tests/integration

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/ml-pipeline \
            ml-pipeline=nlp-pipeline:${{ github.sha }}
```

---

## ğŸ› ï¸ Configuration

### Environment Variables

Create a `.env` file:
```bash
# Model Configuration
HF_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
CLASSIFIER_TYPE=random_forest
MAX_FEATURES=500

# ChromaDB
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION=ticket_embeddings

# Monitoring
EVIDENTLY_ENABLE=true
DRIFT_THRESHOLD=0.15
PROMETHEUS_PUSH_GATEWAY=http://localhost:9091

# Kubernetes
K8S_NAMESPACE=ml-pipeline
JOB_BACKOFF_LIMIT=3
CRON_SCHEDULE="0 2 * * *"  # Daily at 2 AM
```

### Model Configuration (config/default.yaml)
```yaml
preprocessing:
  lowercase: true
  remove_punctuation: true
  remove_stopwords: true
  min_token_length: 2
  max_sequence_length: 512

embeddings:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  batch_size: 32
  normalize: true
  device: "cpu"  # or "cuda"

classification:
  test_size: 0.2
  random_state: 42
  cv_folds: 5
  algorithms:
    - logistic_regression
    - random_forest
    - svm

monitoring:
  baseline_window: 30  # days
  drift_detection_threshold: 0.15
  report_frequency: "daily"
```

---

## ğŸ“š Documentation

- **[API Documentation](docs/api.md)**: Detailed API reference
- **[Deployment Guide](docs/deployment.md)**: Production deployment instructions
- **[Monitoring Setup](docs/monitoring.md)**: Configure Prometheus & Grafana
- **[Troubleshooting](docs/troubleshooting.md)**: Common issues and solutions

---

## ğŸ§ª Testing

### Run Tests
```bash
# Unit tests
pytest tests/unit -v

# Integration tests
pytest tests/integration -v

# Full test suite with coverage
pytest --cov=scripts --cov-report=html

# Open coverage report
open htmlcov/index.html
```

### Test Data

Sample datasets are available in `tests/fixtures/`:
- `sample_tickets.csv`: 100 labeled tickets for testing
- `reference_data.parquet`: Baseline for drift detection

---

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md).

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run linting
make lint

# Run tests
make test
```

### Branching Strategy

- `main`: Production-ready code
- `develop`: Integration branch for features
- `feature/*`: New features
- `bugfix/*`: Bug fixes
- `hotfix/*`: Critical production fixes

---

## ğŸ“ˆ Roadmap

- [ ] **Q2 2024**: Multi-language support (French, German, Spanish)
- [ ] **Q3 2024**: Real-time streaming pipeline with Apache Kafka
- [ ] **Q4 2024**: Advanced models (BERT, GPT-based classifiers)
- [ ] **2025**: Auto-retraining with reinforcement learning from human feedback

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Hugging Face** for pre-trained transformer models
- **Evidently AI** for drift detection framework
- **Prometheus & Grafana** for monitoring infrastructure
- **ChromaDB** for vector storage solution

---

## ğŸ“ Contact & Support

- **Author**: Your Name
- **Email**: your.email@company.com
- **Project Link**: [https://github.com/your-username/nlp-ticket-classification](https://github.com/your-username/nlp-ticket-classification)

### Issues & Bug Reports

Found a bug? Please open an issue with:
1. Clear description of the problem
2. Steps to reproduce
3. Expected vs actual behavior
4. Environment details (OS, Python version, etc.)

### Questions & Discussions

For general questions, use [GitHub Discussions](https://github.com/your-username/nlp-ticket-classification/discussions).

---

<div align="center">

**â­ If you find this project useful, please consider giving it a star! â­**

Made with â¤ï¸ for the MLOps community

</div>